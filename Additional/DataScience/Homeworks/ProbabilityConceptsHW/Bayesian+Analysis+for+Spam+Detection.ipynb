{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Analysis for Spam Detection\n",
    "## Abstract\n",
    "In this assignment, we will apply the concepts we learned about probability, independent events and conditional probability to create a model for detecting spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Theory\n",
    "In order to get started, we will review some of the theory we need, and also extend that a little bit.\n",
    "\n",
    "### Conditional Probability\n",
    "**Conditional probability** is the probability of an event given that another event has occurred.\n",
    "\n",
    "More formally, if the event **A** is the one we're interested in, and event **B** is known to have ocurred, we write the _probability of A given B_ like this: $P(A|B)$.\n",
    "\n",
    "The key point in conditional probability is that event **B** carries additional information. This information can restrict the event space of **A** and therefore change the probability of all elementary events.\n",
    "\n",
    "#### Example\n",
    "If we throw a fair six-sided dice, there's an equal probability of getting each number: $P(1) = P(2) = \\dots = 1/6$. Suppose that the dice has been thrown, so the result is clear. We don't know what it is (for example, the dice may be hidden from us), but we learn that the number on the dice is even. This will change our probability mass function like this: $P(1) = P(3) = P(5) = 0,\\ P(2) = P(4) = P(6) = 1/3$.\n",
    "\n",
    "Why did we assign $1/3$ to the probabilities? The three remaining numbers have equal probability of showing, so there's a very good reason to think their distribution will look like the original one (which is a **uniform distribution**). Also, our event space now has only three events in it, and the total probability should sum up to 1. That's why we calculate the new probabilites to be equal to $1/3$.\n",
    "\n",
    "**In mathematical notation:**\n",
    "* _Random event:_ throwing a fair, six-sided dice\n",
    "* _Random variable:_ $A = $ The number on the top face\n",
    "* _Event space:_ $A = {1, 2, 3, 4, 5, 6}$\n",
    "* _Probability mass function of A:_ $P(1) = P(2) = \\dots = P(6) = 1/6$\n",
    "* _Condition (additional random variable):_ $B = $ The number is even\n",
    "* _Conditional probability:_ The probability of getting a number given that the number is even: $P(A|B)$\n",
    "* _Probability mass function $P(A|B)$ of A given B:_ $P(1) = P(3) = P(5) = 0,\\ P(2) = P(4) = P(6) = 1/3$\n",
    "\n",
    "#### Mathematical Definition\n",
    "If **A** and **B** are events from the event space, with $P(B) > 0$, the conditional probability is given by\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$,\n",
    "where $P(A \\cap B)$ is the **joint probability** of A and B (the probability of both events occurring).\n",
    "\n",
    "### Independent Events\n",
    "We say that **two events are independent** if knowledge of one does not effect the probability distribution of the other. In this case, the joint probability is 0 and $P(A \\cap B) = P(A).P(B)$. This is where the \"multiplication rule\" in combinatorics comes from. This definition is easily extended using more than two events - all probabilites should be multiplied.\n",
    "\n",
    "### Bayes' (or Bayes's) Theorem\n",
    "Bayes' theorem is a direct consequence of the conditional probability definition. It allows us to \"invert\" probabilities, that is, replace the condition and the output.\n",
    "\n",
    "Let's start from the definition of conditional probability:\n",
    "$$ P(A|B) = \\frac{P(A \\cap B)}{P(B)} $$\n",
    "Multiply both sides by $P(B)$ and rearrange:\n",
    "$$ P(A \\cap B) = P(A|B)P(B) $$\n",
    "But we can also exchange the variables **A** and **B**:\n",
    "$$ P(B \\cap A) = P(B|A)P(A) $$\n",
    "With that, note that $A\\cap B$ and $B \\cap A$ is the same union of events, so these are the same:\n",
    "$$ P(A \\cap B) = P(B|A)P(A) $$\n",
    "If the left sides are equal, the right sides must also be equal, therefore:\n",
    "$$ P(A|B)P(B) = P(B|A)P(A) $$\n",
    "If we divide both sides by $P(B)$ (remember, we postulated that it is $> 0$), we get:\n",
    "$$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "\n",
    "The last equation is known as the **Bayes' theorem** and we just proved it.\n",
    "\n",
    "What it means is, that if two events are related, we can use one to predict the other.\n",
    "\n",
    "### Naïve Bayes Method\n",
    "The name is a little misleading. The method isn't \"naïve\", like it should be really bad. It's called like this because of an assumption it makes about the \"predictors\".\n",
    "\n",
    "We saw that Bayes' formula is a way to get from $P(\\text{evidence}|\\text{outcome})$ to $P(\\text{outcome}|\\text{evidence})$. This allows us to **predict** the outcome of an event _that we haven't already seen_.\n",
    "\n",
    "Because we want to support our experiments (and hypotheses) with strong evidence, in the usual case we have **multiple evidence**. \n",
    "\n",
    "Interactions between events can get really complicated, so we can simplify evenything a lot by \"not allowing\" the events (which make up our evidece) to interact. This means, **we assume that all of the events are independent**. The method is called \"naïve\" because of this assumption, but this is true in a lot of cases.\n",
    "\n",
    "We will use this method in the context of classification.\n",
    "\n",
    "### Machine Learning. Classification\n",
    "**Machine learning** is the interaction of \"out-of-the-box\" algorithms with data. We start by coding some algorithm which knows nothing (cue John Snow joke) prior to its execution. But when it starts working, it \"meets\" data and the data it meets \"teaches\" the algorithm what to do.\n",
    "\n",
    "**Classification** is a form of machine learning. More specifically, it's a form of _supervised learning_ (which means that we have worked with some data before and we can teach our algorithm what's right and what's wrong).\n",
    "\n",
    "Classification uses a dataset of \"attributes\" called **predictors** and a **class**. The algorithm (also called a classifier) has to take all of those predictors and output one of the predefined classes:\n",
    "$$ \\{\\mathbb{attr}_1, \\mathbb{attr}_2, \\dots, \\mathbb{attr}_n\\} \\rightarrow C  $$\n",
    "\n",
    "In order to teach the algorithm, we give it a dataset with known attributes and known classes, so it can figure out how the attributes (predictors) are related to the class. There are many ways to work, but we will discuss only one of them.\n",
    "\n",
    "After our algorithm is ready, we can give it a new set of predictors **without the class** and it will output the class it \"thinks\" is best.\n",
    "\n",
    "### Bayesian Classifier Example\n",
    "In order to understand how the classifier works, let's work through a \"toy\" example thanks to [Stack Overflow](http://stackoverflow.com/questions/10059594/a-simple-explanation-of-naive-bayes-classification)).\n",
    "\n",
    "We have a dataset of fruits. The dataset contains 1000 observations (1000 rows, or 1000 fruits that we have measured and written). We have taken into account the following characteristics:\n",
    "* Whether the fruit is long (0 or 1)\n",
    "* Whether the fruit is sweet (0 or 1)\n",
    "* Whether the fruit is yellow (0 or 1)\n",
    "We are using the following classes: \"Orange\", \"Banana\", \"Other\".\n",
    "\n",
    "The beginning of the dataset looks like this:\n",
    "\n",
    "| Long | Sweet | Yellow | Type   |\n",
    "|------|-------|--------|--------|\n",
    "| 1    | 0     | 0      | Other  |\n",
    "| 0    | 1     | 0      | Orange |\n",
    "| 1    | 1     | 0      | Banana |\n",
    "| 1    | 1     | 1      | Banana |\n",
    "| 0    | 1     | 0      | Other  |\n",
    "\n",
    "#### Learning\n",
    "We have summarized the dataset into a \"pivot table\", like this:\n",
    "\n",
    "| Type   | Long | Not long | Sweet | Not sweet | Yellow | Not yellow |\n",
    "|--------|------|----------|-------|-----------|--------|------------|\n",
    "| Banana | 400  | 100      | 350   | 150       | 450    | 50         |\n",
    "| Orange | 0    | 300      | 150   | 150       | 300    | 0          |\n",
    "| Other  | 100  | 100      | 150   | 50        | 50     | 150        |\n",
    "\n",
    "First, we compute all sums by row and column. Note that we don't simply sum up all the rows but we consider each feature (predictor) separately.\n",
    "\n",
    "| Type   | Long | Not long | Sweet | Not sweet | Yellow | Not yellow | Total |\n",
    "|--------|------|----------|-------|-----------|--------|------------|-------|\n",
    "| Banana | 400  | 100      | 350   | 150       | 450    | 50         | 500   |\n",
    "| Orange | 0    | 300      | 150   | 150       | 300    | 0          | 300   |\n",
    "| Other  | 100  | 100      | 150   | 50        | 50     | 150        | 200   |\n",
    "| Total  | 500  | 500      | 650   | 350       | 800    | 200        | 1000  |\n",
    "\n",
    "From that, we can calculate the following things:\n",
    "**Prior probabilities:** They will be our \"starting point\". If we don't know anything else about the fruits, this will be our guess. These are the probabilities to get **one concrete type**. They are calculated using the _row totals_.\n",
    "$$ P(\\text{banana}) = \\text{bananas} / \\text{all fruit} = 500 / 1000 = 0.5 $$\n",
    "$$ P(\\text{orange}) = 0.3 $$\n",
    "$$ P(\\text{other}) = 0.2 $$\n",
    "\n",
    "**Probabilities of evidence:** These are calculated the same way, but regarding each attribute (using the _column totals_).\n",
    "$$ P(\\text{long}) = 500 / 1000 = 0.5 $$\n",
    "$$ P(\\text{not long}) = 0.5 $$\n",
    "$$ P(\\text{sweet}) = 0.6 $$\n",
    "and so on.\n",
    "\n",
    "**Probabilities of likelihood:** These are our _conditional probabilities_. They express the statement \"how many fruits of the given class have that attribute\", in other words, what's the probability of an attribute given the class. They are calculated using the _cell values_.\n",
    "$$ P(\\text{long}|\\text{banana}) = 400 / 500 = 0.8 $$\n",
    "$$ P(\\text{not long}|\\text{banana}) = 0.2$$\n",
    "$$ P(\\text{sweet}|\\text{banana}) = 0.7$$\n",
    "and so on.\n",
    "\n",
    "Now that we have calculated all of these, our **learning phase** is ready and we can proceed to classification.\n",
    "\n",
    "#### Testing\n",
    "We are given a new fruit which looks like this:\n",
    "\n",
    "| Long | Sweet | Yellow | Type   |\n",
    "|------|-------|--------|--------|\n",
    "| 1    | 1     | 1      | ?      |\n",
    "\n",
    "How can we classify it?\n",
    "\n",
    "We can apply the Bayes' formula directly. For all of the classes, we will calculate the probability that our fruit is in one of them.\n",
    "\n",
    "$$ P(\\text{banana}|\\text{long, sweet, yellow}) = \\frac{P(\\text{long}|\\text{banana})P(\\text{sweet}|\\text{banana})P(\\text{yellow}|\\text{banana}).P(\\text{banana})}{P(\\text{long})P(\\text{sweet})P(\\text{yellow})} $$\n",
    "\n",
    "For convenience, let's say that the total probability of evidence is $P_e = P(\\text{long})P(\\text{sweet})P(\\text{yellow})$. \n",
    "\n",
    "We substitute with the values we got from the cells, rows and columns, and we get\n",
    "$$ P(\\text{banana}|\\text{long, sweet, yellow}) = \\frac{0.8 \\times 0.7 \\times 0.9 \\times 0.5}{P_e} = 0.252 / P_e $$\n",
    "\n",
    "We can proceed to calculate the other probabilities.\n",
    "$$ P(\\text{orange}|\\text{long, sweet, yellow}) = 0 $$\n",
    "$$ P(\\text{other}|\\text{long, sweet, yellow}) = 0.01875 / P_e $$\n",
    "\n",
    "We need to compare the three probabilities, so we might just skip calculating the denominator and compare the numerators.\n",
    "\n",
    "We can easily see that the highest probability goes to \"banana\" (by a large margin). So, our classifier will classify that fruit as a banana.\n",
    "\n",
    "| Long | Sweet | Yellow | Type   |\n",
    "|------|-------|--------|--------|\n",
    "| 1    | 1     | 1      | banana |\n",
    "\n",
    "#### Pros and Cons\n",
    "The biggest advantage of this method is that it only involves simple arithmetic. This makes it really fast and suitable for real-time applications. It's also very robust and useful in cases where the attributes aren't correlated.\n",
    "\n",
    "A disadvantage of the method can be seen when data is correlated in some ways. Also, while it's a good classifier, it's not the best estimator (which means that we had better not rely on that method to get a **numerical estimation** instead of a class).\n",
    "\n",
    "Having all of these taken into account, it's still a very popular algorithm. A place where it performs very well is detecting spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Spam Messages\n",
    "Similar to the fruits example, we have a dataset of emails. Each email has been marked as either \"spam\" or \"not spam\" (also called \"ham\"; look for the origin of the word \"spam\" if you don't know why that is). These are our classes.\n",
    "\n",
    "During training, we will explore the emails and see how \"spammy\" every word is. That is, we will calculate the probability that we see a particular word (W) if the message is spam (S): $P(W|S)$. From that, using the Bayes' method, we will get a new email, and calculate the probability of it being spam, given the contents: $P(S|W)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading and Extracting Raw Files\n",
    "We will be working with the most popular email dataset - the one created by Enron. It's located [here](http://www.aueb.gr/users/ion/data/enron-spam/). We'll be using the \"raw\" versions of the messages. Read the \"readme.txt\" file in order to get familiar with the structure and contents of the folders and files.\n",
    "\n",
    "We'll use Python to download and extract the files. We can use the dataset locally, but in this case it's better to download it so that we won't have to supply the dataset with our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been loaded\n"
     ]
    }
   ],
   "source": [
    "urls = {\n",
    "    \"ham\": [\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/ham/beck-s.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/ham/farmer-d.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/ham/kaminski-v.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/ham/kitchen-l.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/ham/lokay-m.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/ham/williams-w3.tar.gz\"],\n",
    "    \"spam\": {\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/spam/BG.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/spam/GP.tar.gz\",\n",
    "        \"http://www.aueb.gr/users/ion/data/enron-spam/raw/spam/SH.tar.gz\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create the folder structure and download the files to their respective locations.\n",
    "# This may take some time, depending on your internet connection\n",
    "base_folder = \"data\"\n",
    "if not os.path.exists(base_folder):\n",
    "    for doc_type, type_urls in urls.items():\n",
    "        type_folder = os.path.join(base_folder, doc_type)\n",
    "        if not os.path.exists(type_folder):\n",
    "            os.makedirs(type_folder)\n",
    "        for url in type_urls:\n",
    "            file_name = os.path.basename(url)\n",
    "            full_path = os.path.join(type_folder, file_name)\n",
    "            \n",
    "            # Download the file\n",
    "            urllib.request.urlretrieve(url, full_path)\n",
    "            \n",
    "            # Extract the contents\n",
    "            tar = tarfile.open(full_path, \"r:gz\")\n",
    "            tar.extractall(type_folder)\n",
    "            tar.close()\n",
    "            \n",
    "            # Delete the archive file\n",
    "            os.remove(full_path)\n",
    "print(\"All files have been loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Files for Modelling\n",
    "In order to prepare all files in memory, we will use a recursive function to traverse all of the files and load their contents. According to the email protocol, the headers and the body of the email are separated by a single blank line. You can see this if you open several email with a text editor. Also, since the emails are (mostly) in English, we load them with \"latin-1\" encoding. Some characters have accents but they are too few to be significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_emails(base_folder):\n",
    "    \"\"\"\n",
    "    Reads all emails in the given base folder, disregarding the inner folder structure. \n",
    "    Returns the paths and email contents, without headers\n",
    "    \"\"\"    \n",
    "    for base_folder, folders, files in os.walk(base_folder):\n",
    "        # Call the function recursively for all folders\n",
    "        for folder in folders:\n",
    "            read_emails(os.path.join(base_folder, folder))\n",
    "        for file in files:\n",
    "            file_path = os.path.join(base_folder, file)\n",
    "            if os.path.isfile(file_path):\n",
    "                body_started = False\n",
    "                lines = []\n",
    "                \n",
    "                # Read the file, skipping the header\n",
    "                f = open(file_path, encoding = \"latin-1\")\n",
    "                for line in f:\n",
    "                    if body_started:\n",
    "                        lines.append(line)\n",
    "                    elif line == \"\\n\":\n",
    "                        body_started = True\n",
    "                f.close()\n",
    "                content = \"\\n\".join(lines)\n",
    "                yield file_path, content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll create a pandas DataFrame object from these emails. This will make it easier for the machine learning library to process. We'll use the file path as index (instead of numbers).\n",
    "\n",
    "After we load the dataframe, we need to shuffle it because if we don't, we'll get the emails in order (they have been ordered by employee and topic). This will be a problem when we try to split the data or perform cross-validation because these rely on getting a random sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52076 emails loaded.\n"
     ]
    }
   ],
   "source": [
    "def create_email_dataframe(base_folder):\n",
    "    \"\"\"\n",
    "    Creates a pandas DataFrame object with all emails in the specified base folder\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    index = []\n",
    "    for file_path, content in read_emails(base_folder):\n",
    "        # The class corresponds to the folder name\n",
    "        current_class = file_path.split(os.sep)[1]\n",
    "        rows.append({ \"text\": content, \"class\": current_class })\n",
    "        index.append(file_path)\n",
    "    \n",
    "    emails = pd.DataFrame(rows, index = index)\n",
    "    emails = emails.reindex(np.random.permutation(emails.index))\n",
    "    return emails\n",
    "\n",
    "emails = create_email_dataframe(\"data\")\n",
    "print (str(len(emails)) + \" emails loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features\n",
    "We have to take all the words inside these emails and count the number of ocurrences of each word. To do this, we're going to use a [\"bag of words\"](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "The machine learning library we'll be using, **scikit-learn**, provides a model like this out of the box. It's called a **CountVectorizer**. You can see its docs [here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "word_counts = count_vectorizer.fit_transform(emails.text.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Classifier\n",
    "Now we're ready to run our classifier. **scikit-learn** also takes care of the algorithm so we don't have to write it from scratch: we can just import it and use it straight away. The classifier takes the features (words) and labels (classes) as two different function parameters.\n",
    "\n",
    "**Note:** We're going to use the multinomial version of the Bayesian classifier because we can't assume that the emails are normally distributed (except multinomial, there are also Bernoulli and Gaussian versions, each of them assuming its own distribution). Also, this is the most common way to use it. Have a look at the [docs](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html#sklearn.naive_bayes.MultinomialNB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "labels = emails[\"class\"].values\n",
    "classifier.fit(word_counts, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Testing\n",
    "Well, that might not seem like much but our classifier is ready and trained.\n",
    "\n",
    "Like in the fruits example, we can try to classify some new emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n"
     ]
    }
   ],
   "source": [
    "# The examples are parts of real emails in the dataset\n",
    "examples = [\n",
    "    \"Hi, let me tell you about the project I was working on. It is ready now.\",\n",
    "    \"The world most effecctiive male enhaancement pi1l\"\n",
    "]\n",
    "\n",
    "example_counts = count_vectorizer.transform(examples)\n",
    "predictions = classifier.predict(example_counts)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Problems\n",
    "Now that you've got a working classifier, you can do several things.\n",
    "\n",
    "**Note:** These are not required as part of your homework. If you have time and desire, research how to do them and try implementing them.\n",
    "\n",
    "* **Training / Testing data:** We can split the data into two parts: one for training and one for testing. In machine learning it's very important that <span style=\"color: red; font-weight: bold\">we test the algorithm with a different dataset than the one used for training</span>. The algorithm has to see something new, otherwise you can't be sure whether it's making its best guess or it has learned all of the answers. This also has to do with overfitting.\n",
    "* **Quantifying results:** How well does your algorithm perform? We will see various estimators during the course. Try searching for \"confusion matrix\", how to get the classifier's confusion matrix and how to interpret it.\n",
    "* **Cross-validation:** This method will allow you to see how \"stable\" your classifier is. In the best case, when you vary the input data, the classification result shouldn't vary too much - this means your classifier is \"stable\". If it is highly sensitive to input data, you'd better change something or use a different algorithm.\n",
    "* **Using n-grams:** N-grams are series of N words next to each other. How will your algorithm perform if you're using 2-grams? How about using 2 classifiers: one with single words, one with 2-grams and reporting their collective answer? This will introduce you to a version of the Bayesian method related to **random forests**.\n",
    "* **Using other types of text analysis:** In this case, we were using a simple \"bag of words\". A better approach is to use \"frequencies\" instead of occurrences. This will balance out emails in terms of their word count. An algorithm you can look for is called **TF-IDF**. Another thing you can try is called \"stemming\", or removing prefixes, suffixes, etc. This will treat words like \"engaging\" and \"engaged\" as the same word.\n",
    "* **Using the same algorithm on other datasets:** Another famous spam dataset is [SpamAssassin's public corpus](https://spamassassin.apache.org/publiccorpus/). You can try running the same (or similar) code on it as well. [This](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) dataset provides an SMS spam collection. It's a little bit easier to work with because there are no separate text files, all of the data is inside one file.\n",
    "* **Training the algorithm on multiple datasets:** You can even train your algorithm on all these, or even more datasets. In general, getting new data means a better learning algorithm.\n",
    "* **Inspecting how the algorithm works:** If you're willing to try different datasets, sometimes adding new data will throw the algorithm off instead of improving it. Research how to add **regularization** and how to plot the algorithm's performance.\n",
    "* **Whatever you like:** Well, I had to add this option as well :)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
